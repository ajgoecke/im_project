# intall necessary packages
#install.packages("quanteda")
#install.packages("readtext")
#install.packages("tidyverse")
#install.packages("quanteda.textstats")
#install.packages("quanteda.textplots")
#install.packages("data.table")
#install.packages("stringr")
#install.packages("spacyr")
#install.packages("textcat")
# load libraries
library(quanteda)
library(readtext)
library(tidyverse)
library(quanteda.textplots)
library(quanteda.textstats)
library(spacyr)
library(stringr)
library(data.table)
library(textcat)
spacy_initialize(model = "de_core_news_sm")
# load packages
packages <- c("jsonlite", "dplyr", "purrr", "quanteda", "readtext")
purrr::walk(packages, library, character.only = TRUE, warn.conflicts = FALSE)
# load packages
packages <- c("jsonlite", "dplyr", "purrr", "quanteda", "readtext")
purrr::walk(packages, library, character.only = TRUE, warn.conflicts = FALSE)
# load json from url or file
#data <- jsonlite::fromJSON("glossary.json")
data <- jsonlite::fromJSON("http://www.klimadiskurs.info/download/json")
# convert json to data frame
glossary.tibble <- as_tibble(data, validate = F) #first turn into tibble
# add new column if compound word not already in glossary
if(word %in% colnames(glossary.tibble))
{warning("Kompositum existiert schon!");
} else {
glossary.tibble[word] <- NA}
# load json from url or file
data <- jsonlite::fromJSON("../files/glossary.json")
# load packages
packages <- c("jsonlite", "dplyr", "purrr", "quanteda", "readtext")
purrr::walk(packages, library, character.only = TRUE, warn.conflicts = FALSE)
# load json from url or file
data <- jsonlite::fromJSON("../files/glossary.json")
#data <- jsonlite::fromJSON("http://www.klimadiskurs.info/download/json")
# convert json to data frame
glossary.tibble <- as_tibble(data, validate = F) #first turn into tibble
# add new column if compound word not already in glossary
if(word %in% colnames(glossary.tibble))
{warning("Kompositum existiert schon!");
} else {
glossary.tibble[word] <- NA}
#insert word here
word = "Klimakonferenz"
#insert word here
word = "Klimakonferenz"
# load packages
packages <- c("jsonlite", "dplyr", "purrr", "quanteda", "readtext")
purrr::walk(packages, library, character.only = TRUE, warn.conflicts = FALSE)
# load json from url or file
data <- jsonlite::fromJSON("../files/glossary.json")
#data <- jsonlite::fromJSON("http://www.klimadiskurs.info/download/json")
# convert json to data frame
glossary.tibble <- as_tibble(data, validate = F) #first turn into tibble
# add new column if compound word not already in glossary
if(word %in% colnames(glossary.tibble))
{warning("Kompositum existiert schon!");
} else {
glossary.tibble[word] <- NA}
# convert tibble to data frame
glossary.df <- as.data.frame(do.call("cbind", glossary.tibble))
# load corpus files
contra2000 <- readRDS("../corpus_data/contra2000.rds")
pro2000 <- readRDS("../corpus_data/pro2000.rds")
# create "sentence" tokens for p2000 and c2000 corpus
p2000_toks.sent <- tokens(pro2000, remove_punct = FALSE, remove_symbols = TRUE,
remove_numbers = TRUE, remove_url = TRUE, remove_separators = TRUE,
what = "sentence")
c2000_toks.sent <- tokens(contra2000, remove_punct = FALSE, remove_symbols = TRUE,
remove_numbers = TRUE, remove_url = TRUE, remove_separators = TRUE,
what = "sentence")
# load packages
packages <- c("jsonlite", "dplyr", "purrr", "quanteda", "readtext")
purrr::walk(packages, library, character.only = TRUE, warn.conflicts = FALSE)
# load json from url or file
#data <- jsonlite::fromJSON("../files/glossary.json")
data <- jsonlite::fromJSON("http://www.klimadiskurs.info/download/json")
# convert json to data frame
glossary.tibble <- as_tibble(data, validate = F) #first turn into tibble
# add new column if compound word not already in glossary
if(word %in% colnames(glossary.tibble))
{warning("Kompositum existiert schon!");
} else {
glossary.tibble[word] <- NA}
# convert tibble to data frame
glossary.df <- as.data.frame(do.call("cbind", glossary.tibble))
# load corpus files
contra2000 <- readRDS("../corpus_data/contra2000.rds")
pro2000 <- readRDS("../corpus_data/pro2000.rds")
# create "sentence" tokens for p2000 and c2000 corpus
p2000_toks.sent <- tokens(pro2000, remove_punct = FALSE, remove_symbols = TRUE,
remove_numbers = TRUE, remove_url = TRUE, remove_separators = TRUE,
what = "sentence")
c2000_toks.sent <- tokens(contra2000, remove_punct = FALSE, remove_symbols = TRUE,
remove_numbers = TRUE, remove_url = TRUE, remove_separators = TRUE,
what = "sentence")
# add term, i.e. word itself
glossary.df['term', word] <- word
# add id (length of dataframe since we append it to the tail)
glossary.df['id', word] <-ncol(glossary.df)
# add definition, empty string if no definition available
glossary.df['definition', word] <- " "
# add sources, empty list if no sources available
glossary.df['sources', word][[1]] <- list(list())
# add related words, empty list if no related words available
glossary.df['related', word][[1]] <- list(list())
# add spellings, i.e. word itself and version with hyphen
hyp <- gsub('(.{5})(.*)', '\\1-\\2', word) # put hyphen after "Klima"
hyp <- gsub("(^|-)([[:alpha:]])", "\\1\\U\\2", hyp, perl=TRUE) # convert first letter of second noun part to upper case
glossary.df['spellings', word][[1]] <- list(c(word,hyp))
# add examples
# check keyword in context for each corpus
pro_kwic <- kwic(p2000_toks.sent, word, valuetype = 'regex', window = 1)
con_kwic <- kwic(c2000_toks.sent, word, valuetype = 'regex', window = 1)
# get random sample sentences (2 each) that contains the keyword
try(pro_sents <- pro_kwic[sample(nrow(pro_kwic), 2), ]$keyword, silent=TRUE)
try(con_sents <- con_kwic[sample(nrow(con_kwic), 2), ]$keyword, silent = TRUE)
glossary.df['examples', word][[1]] <- list(c(pro_sents,con_sents))
# add association, if keyword not found in pro/contra corpus, do not add id
association <- c()
if(exists("con_sents") == TRUE){
association <- append(association,0)} # add 0 if keyword found in contra corpus
if(exists("pro_sents") == TRUE){
association <- append(association,1)} # add 1 if keyword found in pro corpus
glossary.df['association', word][[1]] <- list(association)
# save data frame to json file
glossary_list <- as.list(glossary.df) # first convert to list
glossary_json <- jsonlite::toJSON(glossary_list, pretty=TRUE, auto_unbox=TRUE, rownames=TRUE) # save to json
write(glossary_json, "glossary_new.json") # write to directory
# load packages
packages <- c("jsonlite", "dplyr", "purrr", "quanteda", "readtext")
purrr::walk(packages, library, character.only = TRUE, warn.conflicts = FALSE)
# load json from url or file
#data <- jsonlite::fromJSON("../files/glossary.json")
data <- jsonlite::fromJSON("http://www.klimadiskurs.info/download/json")
# convert json to data frame
glossary.tibble <- as_tibble(data, validate = F) #first turn into tibble
# add new column if compound word not already in glossary
if(word %in% colnames(glossary.tibble))
{warning("Kompositum existiert schon!");
} else {
glossary.tibble[word] <- NA}
# convert tibble to data frame
glossary.df <- as.data.frame(do.call("cbind", glossary.tibble))
# load corpus files
contra2000 <- readRDS("../corpus_data/contra2000.rds")
pro2000 <- readRDS("../corpus_data/pro2000.rds")
# create "sentence" tokens for p2000 and c2000 corpus
p2000_toks.sent <- tokens(pro2000, remove_punct = FALSE, remove_symbols = TRUE,
remove_numbers = TRUE, remove_url = TRUE, remove_separators = TRUE,
what = "sentence")
c2000_toks.sent <- tokens(contra2000, remove_punct = FALSE, remove_symbols = TRUE,
remove_numbers = TRUE, remove_url = TRUE, remove_separators = TRUE,
what = "sentence")
# create a sample of the dfm with all words starting with "klimaX"
klima_p2000 <- dfm_select(dfm_p2000_lemma, pattern="klima*")
# intall necessary packages
#install.packages("quanteda")
#install.packages("readtext")
#install.packages("tidyverse")
#install.packages("quanteda.textstats")
#install.packages("quanteda.textplots")
#install.packages("data.table")
#install.packages("stringr")
#install.packages("spacyr")
#install.packages("textcat")
# load libraries
library(quanteda)
library(readtext)
library(tidyverse)
library(quanteda.textplots)
library(quanteda.textstats)
library(spacyr)
library(stringr)
library(data.table)
library(textcat)
spacy_initialize(model = "de_core_news_sm")
# load corpus files
pro2000 = readRDS("../corpus_data/corpora/pro2000.rds")
# load corpus files
pro2000 = readRDS("../corpus_data/corpora/pro2000.rds")
# load list of final climate change compounds
compounds <- scan("../files/wordlist.txt", what="", sep="\n")
# load list of final climate change compounds and word forms
compound_df <- read.csv("../files/compound.csv", sep = ";", stringsAsFactors=FALSE)
# load list of final climate change compounds and word forms
compound_df <- read.csv("../files/compounds.csv", sep = ";", stringsAsFactors=FALSE)
# lower compounds and word forms
compounds <- tolower(compounds)
compound_df$original <- tolower(compound_df$original)
compound_df$compound_forms <- tolower(compound_df$compound_forms)
compound_df
# load list of final climate change compounds and word forms
compound_df <- read.csv("../files/compounds.csv", sep = ";", stringsAsFactors=FALSE)
compound_df
# load list of final climate change compounds and word forms
compound_df <- read.csv("../files/compounds.csv", sep = ";", stringsAsFactors=FALSE)
compound_df
# lower compounds and word forms
compounds <- tolower(compounds)
compound_df$original <- tolower(compound_df$original)
compound_df$compound_forms <- tolower(compound_df$compound_forms)
compound_df
compound_df
# retrieve stoplists
de_stopwords <- stopwords::stopwords("de", source="snowball")
en_stopwords <- stopwords::stopwords("en", source="snowball" )
# add custom stoplist
custom_stopwords <- c("dass", "=", "the", "seit", "ab", "beim", "\n", "mal", "c", "\\|","|", "m", "kommentare", "neueste", "gepostet", "admin", "cookies", "inhalte", "inhalt", "newsletter", "posten", "zugriff", "passwort", "geschützt", "seite", "website", "webseite", "and", "0", "1", "2", "3","4","5","6","7","8","9", "mfg","w","t","wer","00", "30", ">", "anmelden", "\\+", "40", "81", "erneuerbarer",
"OWLIT", "et", "\\´", "\\^", "tppubtype", "pubstate", "z", "b", "d", "ct", "--")
# combine lists to a full version
full_stopwords <- c(de_stopwords,en_stopwords,custom_stopwords)
load("~/Documents/uni/im_glossary/workspace.RDataTmp.RData")
View(sp_pro2000)
# parse the pro corpus with spacy function and retrieve lemma for each token
sp_pro2000 <- spacy_parse(pro2000, pos=FALSE, entity=FALSE, dependency=FALSE)
sp_contra2000 <- spacy_parse(contra2000, pos=FALSE, entity=FALSE, dependency=FALSE)
# function to preprocess compounds data frame
unlist_forms = function(word){
x <- unlist(strsplit(word, ","))
return(gsub(" ","",x))}
# retrieve lemma forms for our compound words
# for each compound word
for (words in compound_df$compound_forms){
word_forms = c(unlist_forms(words)) # put into correct format
# each word form of the compound word
for (token in word_forms){
# retrieve the lemma form
lemma_form <- compound_df[compound_df$compound_forms %like% token, ]$original[[1]]
# replace lemma form in pro and contra lemmatized data frame
# to make sure we get correct lemma form
sp_pro2000$lemma[tolower(sp_pro2000$token) == token] <- lemma_form
sp_contra2000$lemma[tolower(sp_contra2000$token) == token] <- lemma_form
}
}
# replace token with lemma - Pro Corpus
sp_pro2000$token <- sp_pro2000$lemma
# replace token with lemma - Contra Corpus
sp_contra2000$token <- sp_contra2000$lemma
# remove hyphens from tokens
sp_p2000_tokens <- as.tokens(sp_pro2000)
toks_comp <- tokens_compound(sp_p2000_tokens, phrase("*-*"), concatenator ="")
toks_hyphenated <- grep("\\w+-\\w+", types(toks_comp), value = TRUE)
sp_p2000_tokens <- tokens_replace(toks_comp, toks_hyphenated, gsub("-", "", toks_hyphenated))
sp_c2000_tokens <- as.tokens(sp_contra2000)
toks_comp <- tokens_compound(sp_c2000_tokens, phrase("*-*"), concatenator ="")
toks_hyphenated <- grep("\\w+-\\w+", types(toks_comp), value = TRUE)
sp_c2000_tokens <- tokens_replace(toks_comp, toks_hyphenated, gsub("-", "", toks_hyphenated))
# apply preprocessing to pro corpus tokens
sp_p2000_tokens <- sp_p2000_tokens %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE,
remove_separators = TRUE, split_hyphens=FALSE) %>%
tokens_tolower() %>%
tokens_remove(pattern = full_stopwords, padding = FALSE)
# apply preprocessing to contra corpus tokens
sp_c2000_tokens <- sp_c2000_tokens %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE,
remove_separators = TRUE, split_hyphens=FALSE) %>%
tokens_tolower() %>%
tokens_remove(pattern = full_stopwords, padding = FALSE)
# create dfm
dfm_p2000_lemma <- dfm(sp_p2000_tokens)
dfm_c2000_lemma <- dfm(sp_c2000_tokens)
# min/max normalization from -1 to 1, relative to data frame results
normalize <- function(x, na.rm = TRUE){
return((x - min(x)) / (max(x)-min(x)))}
# create a sample of the dfm with all words starting with "klimaX"
klima_p2000 <- dfm_select(dfm_p2000_lemma, pattern="klima*")
klima_c2000 <- dfm_select(dfm_c2000_lemma, pattern="klima*")
# calculate tfidf for "klima" words
p2000_tfidf <- dfm_tfidf(klima_p2000, scheme_tf = "prop", scheme_df = "inverse")
c2000_tfidf <- dfm_tfidf(klima_c2000, scheme_tf = "prop", scheme_df = "inverse")
# retrieve frequencies for "klima" words
freqs_pro <- textstat_frequency(p2000_tfidf, force=TRUE)
freqs_con <- textstat_frequency(c2000_tfidf, force=TRUE)
# apply normalization
freqs_pro$normalize = round(normalize(freqs_pro$frequency), 3)
freqs_con$normalize = round(normalize(freqs_con$frequency), 3)
# subset for words that are contained in our final glossary list
freqs_pro_subset <- freqs_pro[freqs_pro$feature %in% compounds, ]
freqs_pro_subset$feature <- str_to_title(freqs_pro_subset$feature)
freqs_con_subset <- freqs_con[freqs_con$feature %in% compounds, ]
freqs_con_subset$feature <- str_to_title(freqs_con_subset$feature)
# plot comparison of this subset for both groups
freqs.act <- filter(freqs_pro_subset) %>% as.data.frame() %>% select(feature, normalize)
freqs.scept <- filter(freqs_con_subset) %>% as.data.frame() %>% select(feature, normalize)
freqs <- left_join(freqs.act, freqs.scept, by = "feature") %>% head(30) %>% arrange(normalize.x) %>% mutate(feature = factor(feature, feature))
# create plot
plot8 <- ggplot(freqs) +
geom_segment(aes(x=feature, xend=feature, y=normalize.x, yend=normalize.y), color="grey") +
geom_point(aes(x=feature, y=normalize.x, colour="Activists"), size = 3) +
geom_point(aes(x=feature, y=normalize.y, colour="Sceptics"), size = 3) +
ggtitle("Comparison 'Klima' TF-IDF Scores per Group") +
xlab("") + ylab("TF-IDF") +
coord_flip()
plot8+labs(colour="Group")
# create plot
plot8 <- ggplot(freqs) +
geom_segment(aes(x=feature, xend=feature, y=normalize.x, yend=normalize.y), color="grey") +
geom_point(aes(x=feature, y=normalize.x, colour="Activists"), size = 3) +
geom_point(aes(x=feature, y=normalize.y, colour="Sceptics"), size = 3) +
ggtitle("Comparison 'Klima' TF-IDF Scores per Group") +
xlab("") + ylab("TF-IDF") +
coord_flip()
plot8+labs(colour="Group")
plot8
library("ggplot2")
# create a sample of the dfm with all words starting with "klimaX"
klima_p2000 <- dfm_select(dfm_p2000_lemma, pattern="klima*")
klima_c2000 <- dfm_select(dfm_c2000_lemma, pattern="klima*")
# calculate tfidf for "klima" words
p2000_tfidf <- dfm_tfidf(klima_p2000, scheme_tf = "prop", scheme_df = "inverse")
c2000_tfidf <- dfm_tfidf(klima_c2000, scheme_tf = "prop", scheme_df = "inverse")
# retrieve frequencies for "klima" words
freqs_pro <- textstat_frequency(p2000_tfidf, force=TRUE)
freqs_con <- textstat_frequency(c2000_tfidf, force=TRUE)
# apply normalization
freqs_pro$normalize = round(normalize(freqs_pro$frequency), 3)
freqs_con$normalize = round(normalize(freqs_con$frequency), 3)
# subset for words that are contained in our final glossary list
freqs_pro_subset <- freqs_pro[freqs_pro$feature %in% compounds, ]
freqs_pro_subset$feature <- str_to_title(freqs_pro_subset$feature)
freqs_con_subset <- freqs_con[freqs_con$feature %in% compounds, ]
freqs_con_subset$feature <- str_to_title(freqs_con_subset$feature)
# plot comparison of this subset for both groups
freqs.act <- filter(freqs_pro_subset) %>% as.data.frame() %>% select(feature, normalize)
freqs.scept <- filter(freqs_con_subset) %>% as.data.frame() %>% select(feature, normalize)
freqs <- left_join(freqs.act, freqs.scept, by = "feature") %>% head(30) %>% arrange(normalize.x) %>% mutate(feature = factor(feature, feature))
# create plot
plot8 <- ggplot(freqs) +
geom_segment(aes(x=feature, xend=feature, y=normalize.x, yend=normalize.y), color="grey") +
geom_point(aes(x=feature, y=normalize.x, colour="Activists"), size = 3) +
geom_point(aes(x=feature, y=normalize.y, colour="Sceptics"), size = 3) +
ggtitle("Comparison 'Klima' TF-IDF Scores per Group") +
xlab("") + ylab("TF-IDF") +
coord_flip()
plot8
# create plot
plot8 <- ggplot(freqs) +
geom_segment(aes(x=feature, xend=feature, y=normalize.x, yend=normalize.y), color="grey") +
geom_point(aes(x=feature, y=normalize.x, colour="Activists"), size = 3) +
geom_point(aes(x=feature, y=normalize.y, colour="Sceptics"), size = 3) +
ggtitle("Comparison 'Klima' TF-IDF Scores per Group") +
xlab("") + ylab("TF-IDF") +
coord_flip()
plot8+labs(colour="Group")
# create a sample of the dfm with all words starting with "klimaX"
klima_p2000 <- dfm_select(dfm_p2000_lemma, pattern="klima*")
klima_c2000 <- dfm_select(dfm_c2000_lemma, pattern="klima*")
# calculate tfidf for "klima" words
p2000_tfidf <- dfm_tfidf(klima_p2000, scheme_tf = "prop", scheme_df = "inverse")
c2000_tfidf <- dfm_tfidf(klima_c2000, scheme_tf = "prop", scheme_df = "inverse")
# retrieve frequencies for "klima" words
freqs_pro <- textstat_frequency(p2000_tfidf, force=TRUE)
freqs_con <- textstat_frequency(c2000_tfidf, force=TRUE)
# apply normalization
freqs_pro$normalize = round(normalize(freqs_pro$frequency), 3)
freqs_con$normalize = round(normalize(freqs_con$frequency), 3)
# subset for words that are contained in our final glossary list
freqs_pro_subset <- freqs_pro[freqs_pro$feature %in% compounds, ]
freqs_pro_subset$feature <- str_to_title(freqs_pro_subset$feature)
freqs_con_subset <- freqs_con[freqs_con$feature %in% compounds, ]
freqs_con_subset$feature <- str_to_title(freqs_con_subset$feature)
# plot comparison of this subset for both groups
freqs.act <- filter(freqs_pro_subset) %>% as.data.frame() %>% select(feature, normalize)
freqs.scept <- filter(freqs_con_subset) %>% as.data.frame() %>% select(feature, normalize)
freqs <- left_join(freqs.act, freqs.scept, by = "feature") %>% head(30) %>% arrange(normalize.x) %>% mutate(feature = factor(feature, feature))
# create plot
plot8 <- ggplot(freqs) +
geom_segment(aes(x=feature, xend=feature, y=normalize.x, yend=normalize.y)fill=c("red","blue"), color="grey") +
# create plot
plot8 <- ggplot(freqs) +
geom_segment(aes(x=feature, xend=feature, y=normalize.x, yend=normalize.y), fill=c("red","blue"), color="grey") +
geom_point(aes(x=feature, y=normalize.x, colour="Activists"), size = 3) +
geom_point(aes(x=feature, y=normalize.y, colour="Sceptics"), size = 3) +
ggtitle("Comparison 'Klima' TF-IDF Scores per Group") +
xlab("") + ylab("TF-IDF") +
coord_flip()
plot8+labs(colour="Group")
# create a sample of the dfm with all words starting with "klima..."
klima_p2000 <- dfm_select(dfm_p2000, pattern="klima*")
# create a sample of the dfm with all words starting with "klima..."
klima_p2000 <- dfm_select(dfm_p2000_lemma, pattern="klima*")
klima_c2000 <- dfm_select(dfm_c2000_lemma, pattern="klima*")
# calculate tfidf for "klima" words
p2000_tfidf <- dfm_tfidf(klima_p2000, scheme_tf = "prop", scheme_df = "inverse")
c2000_tfidf <- dfm_tfidf(klima_c2000, scheme_tf = "prop", scheme_df = "inverse")
# retrieve frequencies for "klima" words
freqs_pro <- textstat_frequency(p2000_tfidf, force=TRUE)
freqs_con <- textstat_frequency(c2000_tfidf, force=TRUE)
# apply normalization
freqs_pro$normalize = round(normalize(freqs_pro$frequency), 3)
freqs_con$normalize = round(normalize(freqs_con$frequency), 3)
# retrieve only words that are contained in our final compound list
freqs_pro_subset <- freqs_pro[freqs_pro$feature %in% compounds, ]
freqs_pro_subset$feature <- str_to_title(freqs_pro_subset$feature)
freqs_con_subset <- freqs_con[freqs_con$feature %in% compounds, ]
freqs_con_subset$feature <- str_to_title(freqs_con_subset$feature)
# plot comparison of both groups -> only words from compound list
freqs.act <- filter(freqs_pro_subset) %>% as.data.frame() %>% select(feature, normalize)
freqs.scept <- filter(freqs_con_subset) %>% as.data.frame() %>% select(feature, normalize)
freqs <- left_join(freqs.act, freqs.scept, by = "feature") %>% head(30) %>% arrange(normalize.x) %>% mutate(feature = factor(feature, feature))
# create plot
plot8 <- ggplot(freqs) +
geom_segment(aes(x=feature, xend=feature, y=normalize.x, yend=normalize.y), color="grey") +
geom_point(aes(x=feature, y=normalize.x, colour="Activists"), size = 3) +
geom_point(aes(x=feature, y=normalize.y, colour="Sceptics"), size = 3) +
ggtitle("Comparison 'Klima' TF-IDF Scores per Group") +
xlab("") + ylab("TF-IDF") +
coord_flip()
plot8+labs(colour="Group")
# create a sample of the dfm with all words starting with "klima..."
klima_p2000 <- dfm_select(dfm_p2000, pattern="klima*")
# create dfm
dfm_p2000 <- dfm(pro2000_tokens)
dfm_c2000 <- dfm(contra2000_tokens)
# create a sample of the dfm with all words starting with "klima..."
klima_p2000 <- dfm_select(dfm_p2000, pattern="klima*")
klima_c2000 <- dfm_select(dfm_c2000, pattern="klima*")
# calculate tfidf for "klima" words
p2000_tfidf <- dfm_tfidf(klima_p2000, scheme_tf = "prop", scheme_df = "inverse")
c2000_tfidf <- dfm_tfidf(klima_c2000, scheme_tf = "prop", scheme_df = "inverse")
# retrieve frequencies for "klima" words
freqs_pro <- textstat_frequency(p2000_tfidf, force=TRUE)
freqs_con <- textstat_frequency(c2000_tfidf, force=TRUE)
# apply normalization
freqs_pro$normalize = round(normalize(freqs_pro$frequency), 3)
freqs_con$normalize = round(normalize(freqs_con$frequency), 3)
# retrieve only words that are contained in our final compound list
freqs_pro_subset <- freqs_pro[freqs_pro$feature %in% compounds, ]
freqs_pro_subset$feature <- str_to_title(freqs_pro_subset$feature)
freqs_con_subset <- freqs_con[freqs_con$feature %in% compounds, ]
freqs_con_subset$feature <- str_to_title(freqs_con_subset$feature)
# plot comparison of both groups -> only words from compound list
freqs.act <- filter(freqs_pro_subset) %>% as.data.frame() %>% select(feature, normalize)
freqs.scept <- filter(freqs_con_subset) %>% as.data.frame() %>% select(feature, normalize)
freqs <- left_join(freqs.act, freqs.scept, by = "feature") %>% head(30) %>% arrange(normalize.x) %>% mutate(feature = factor(feature, feature))
# create plot
plot8 <- ggplot(freqs) +
geom_segment(aes(x=feature, xend=feature, y=normalize.x, yend=normalize.y), color="grey") +
geom_point(aes(x=feature, y=normalize.x, colour="Activists"), size = 3) +
geom_point(aes(x=feature, y=normalize.y, colour="Sceptics"), size = 3) +
ggtitle("Comparison 'Klima' TF-IDF Scores per Group") +
xlab("") + ylab("TF-IDF") +
coord_flip()
plot8+labs(colour="Group")
